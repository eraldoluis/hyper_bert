{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BERT base-uncased\n",
    "## Usando os 2 melhores padrões do HypeNet-train\n",
    "\n",
    "### SEP usa esses 2 e faz uma permutação gerando 15 padrões 2 a 2.\n",
    "\n",
    "### DOT usa esses 2 e faz uma permutação de tamnho 2 até 2 gerando 2 padrões separado com um ponto.\n",
    "> Suponha o par (3,2) (abacate, fruta) e os padrões (é um tipo de) e (é um)\n",
    "> SEP:\n",
    "- [CLS] [MASK] ca te é um tipo de fru ta [SEP] aba ca te é um fru ta [SEP]\n",
    "- [CLS] aba [MASK] te é um tipo de fru ta [SEP] aba ca te é um fru ta [SEP]\n",
    "- [CLS] aba ca [MASK] é um tipo de fru ta [SEP] aba ca te é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de [MASK] ta [SEP] aba ca te é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru [MASK] [SEP] aba ca te é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru ta [SEP] [MASK] ca te é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru ta [SEP] aba [MASK] te é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru ta [SEP] aba ca [MASK] é um fru ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru ta [SEP] aba ca te é um [MASK] ta [SEP]\n",
    "- [CLS] aba ca te é um tipo de fru ta [SEP] aba ca te é um fru [MASK] [SEP]\n",
    "\n",
    "\n",
    "- Os primeiros gráficos mostram o balanceamento de cada dataset.\n",
    "  - % dos pares True para cada tamanho de subtoken\n",
    "\n",
    "  - Quantidade total de pares para cada tamanho de subtoken\n",
    "\n",
    "- último gŕafico mostra AP dos datasets.\n",
    "\n",
    "\n",
    "> OBS: inconsistência nos datasets turney2014"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kotlerman2010.json normal\n",
      "Balanceamento: [1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "levy2014.json normal\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "turney2014.json normal\n",
      "Balanceamento: [1, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "baroni2012.json normal\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "EVALution.json normal\n",
      "Balanceamento: [1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "LenciBenotto.json normal\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "Weeds.json normal\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "BLESS.json normal\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "wordnet_test.json normal\n",
      "Balanceamento: [1, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "HypeNet_test.json normal\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "kotlerman2010.json dot\n",
      "Balanceamento: [1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "levy2014.json dot\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "turney2014.json dot\n",
      "Balanceamento: [1, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "baroni2012.json dot\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "EVALution.json dot\n",
      "Balanceamento: [1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "LenciBenotto.json dot\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "Weeds.json dot\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "BLESS.json dot\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "wordnet_test.json dot\n",
      "Balanceamento: [1, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "HypeNet_test.json dot\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "kotlerman2010.json sep\n",
      "Balanceamento: [1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "levy2014.json sep\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "turney2014.json sep\n",
      "Balanceamento: [1, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "baroni2012.json sep\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "EVALution.json sep\n",
      "Balanceamento: [1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "LenciBenotto.json sep\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "Weeds.json sep\n",
      "Balanceamento: [1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "BLESS.json sep\n",
      "Balanceamento: [1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "wordnet_test.json sep\n",
      "Balanceamento: [1, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n",
      "HypeNet_test.json sep\n",
      "Balanceamento: [1, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] não está no dataframe!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nb_utils\n",
    "import itertools\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "# path_json_normal = \"../../aaaaaaaa/bert-base-uncased_bert_score_2020-11-12_17:28:09\"\n",
    "# path_json_dot = \"../../aaaaaaaa/bert-base-uncased_bert_score_dot_comb_2020-11-12_17:29:58\"\n",
    "# path_json_sep = \"../../aaaaaaaa/bert-base-uncased_bert_score_sep_comb_2020-11-12_17:29:11\"\n",
    "\n",
    "path_json_normal = \"../../en-model/bert-base-uncased_bert_score_normal_2020-11-12_15:13:01\"\n",
    "path_json_dot = \"../../en-model/bert-base-uncased_bert_score_dot_comb_2020-11-12_14:48:06\"\n",
    "path_json_sep = \"../../en-model/bert-base-uncased_bert_score_sep_comb_2020-11-12_14:48:36\"\n",
    "\n",
    "dfs = {}\n",
    "pattern_unique = {}\n",
    "filename = \"\"\n",
    "for filename in os.listdir(path_json_normal):\n",
    "    if os.path.isfile(os.path.join(path_json_normal, filename)) and filename.endswith(\".json\"):\n",
    "        df = nb_utils.create_dataframe(json.load(open(os.path.join(path_json_normal, filename))), combination=False, separator=\"\\t\")\n",
    "        df['tipo'] = 'normal'\n",
    "        dfs[filename + \" normal\"] = df\n",
    "pattern_unique['normal'] = df['pattern'].unique().tolist()\n",
    "for filename in os.listdir(path_json_dot):\n",
    "    if os.path.isfile(os.path.join(path_json_dot, filename)) and filename.endswith(\".json\"):\n",
    "        df = nb_utils.create_dataframe(json.load(open(os.path.join(path_json_dot, filename))), combination=False, separator=\"\\t\")\n",
    "        df['tipo'] = 'dot'\n",
    "        dfs[filename + \" dot\"] = df\n",
    "pattern_unique['dot'] = df['pattern'].unique().tolist()\n",
    "for filename in os.listdir(path_json_sep):\n",
    "    if os.path.isfile(os.path.join(path_json_sep, filename)) and filename.endswith(\".json\"):\n",
    "        df = nb_utils.create_dataframe(json.load(open(os.path.join(path_json_sep, filename))), combination=True, separator=\"\\t\")\n",
    "        df['tipo'] = 'sep'\n",
    "        dfs[filename + \" sep\"] = df\n",
    "pattern_unique['sep'] = df['pattern'].unique().tolist()\n",
    "\n",
    "len_total_unique = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "\n",
    "df_ratios = []\n",
    "for dataset_name in dfs.keys():\n",
    "    print(dataset_name)\n",
    "    dname, type_dataset = dataset_name.split()\n",
    "    df_ratio = nb_utils.balanceamento(dfs[dataset_name], len_size=len_total_unique, patterns=pattern_unique[type_dataset])\n",
    "    df_ratio['dataset'] = dname\n",
    "    df_ratio['tipo'] = type_dataset\n",
    "    df_ratios.append(df_ratio)\n",
    "df_ratios = pd.concat(df_ratios, ignore_index=True)\n",
    "\n",
    "for dataset_name, df in dfs.items():\n",
    "    type_dataset = dataset_name.split()[-1]\n",
    "    dfs[dataset_name] = nb_utils.logsumexp_normalization(df, len_list=len_total_unique, pattern_list=pattern_unique[type_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ratios['dataset_tipo'] = df_ratios['dataset'] + \" \" + df_ratios['tipo']\n",
    "df_plot_lensubtoken = df_ratios[df_ratios['tipo'] == 'normal']\n",
    "# % dos pares True por tamanho de subtoken\n",
    "g = sns.catplot(x=\"len_total\", y=\"ratio\", col=\"dataset\", col_wrap=5,\n",
    "                data=df_plot_lensubtoken, saturation=.5,\n",
    "                kind=\"bar\", ci=None, aspect=.9, sharey=False)\n",
    "(g.set_axis_labels(\"\", \"% true\")\n",
    "#   .set_xticklabels([\"Men\", \"Women\", \"Children\"])\n",
    "  .set_titles(\"{col_name}\")\n",
    "  .despine(left=False))\n",
    "plt.subplots_adjust(top=0.9, hspace=0.3)\n",
    "_ = g.fig.suptitle('% dos pares verdadeiros em cada dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quantidade de pares por tamanho de subtoken\n",
    "df_ratios['total'] = df_ratios['true'] + df_ratios['false']\n",
    "df_plot_lensubtoken = df_ratios[df_ratios['tipo'] == 'normal']\n",
    "\n",
    "g = sns.catplot(x=\"len_total\", y=\"total\", col=\"dataset\", col_wrap=5,\n",
    "                data=df_plot_lensubtoken, saturation=.5,\n",
    "                kind=\"bar\", ci=None, aspect=.9, sharey=False)\n",
    "(g.set_axis_labels(\"\", \"Num pares\")\n",
    "#   .set_xticklabels([\"Men\", \"Women\", \"Children\"])\n",
    "  .set_titles(\"{col_name}\")\n",
    "  # .set(ylim=(0, 1))\n",
    "  .despine(left=False))\n",
    "plt.subplots_adjust(top=0.9, hspace=0.3)\n",
    "_ = g.fig.suptitle('Quantidade de exemplos em cada dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del nb_utils\n",
    "import nb_utils\n",
    "df_en = []\n",
    "dfs2 = []\n",
    "best_pattern_train = nb_utils.best_pattern_HypeNet_train_logz\n",
    "best_pattern_num_train = 2\n",
    "# parei aqui, melhor jogar esse for detro do nb_utils\n",
    "for dataset_name in dfs.keys():\n",
    "    dname, type_dataset = dataset_name.split()\n",
    "    if type_dataset == 'normal':\n",
    "        df_temp= nb_utils.compute_min_mean_ap_normal(dfs[dataset_name],\n",
    "            pattern_list=best_pattern_train, dataset_name=dname, best_pattern_num=best_pattern_num_train)\n",
    "        df_temp['tipo'] = 'normal'\n",
    "        dfs2.append(df_temp)\n",
    "        df_temp = nb_utils.compute_ap_bert_soma(dfs[dataset_name], pattern_list=best_pattern_train, dataset_name=dname,\n",
    "                                                best_pattern_num=best_pattern_num_train, tipo=type_dataset)\n",
    "        df_temp['tipo'] = \"normal\"\n",
    "        dfs2.append(df_temp)\n",
    "    elif type_dataset == 'dot':\n",
    "        df_temp= nb_utils.compute_min_mean_ap_dot(dfs[dataset_name],\n",
    "            pattern_list=best_pattern_train, dataset_name=dname, best_pattern_num=best_pattern_num_train)\n",
    "        df_temp['tipo'] = 'dot'\n",
    "        dfs2.append(df_temp)\n",
    "        df_temp = nb_utils.compute_ap_bert_soma(dfs[dataset_name], pattern_list=best_pattern_train, dataset_name=dname,\n",
    "                                                best_pattern_num=best_pattern_num_train, tipo=type_dataset)\n",
    "        df_temp['tipo'] = \"dot\"\n",
    "        dfs2.append(df_temp)\n",
    "    elif type_dataset == 'sep':\n",
    "        df_temp= nb_utils.compute_min_mean_ap_sep(dfs[dataset_name],\n",
    "            pattern_list=best_pattern_train, dataset_name=dname, best_pattern_num=best_pattern_num_train)\n",
    "        df_temp['tipo'] = 'sep'\n",
    "        dfs2.append(df_temp)\n",
    "        df_temp = nb_utils.compute_ap_bert_soma(dfs[dataset_name], pattern_list=best_pattern_train, dataset_name=dname,\n",
    "                                                best_pattern_num=best_pattern_num_train, tipo=type_dataset)\n",
    "        df_temp['tipo'] = \"sep\"\n",
    "        dfs2.append(df_temp)\n",
    "    else:\n",
    "        raise KeyError\n",
    "\n",
    "df_en = pd.concat(dfs2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dive = nb_utils.get_df_dive()\n",
    "df_dive['method_format_tipo'] = df_dive['method'].map(nb_utils.method_names)\n",
    "# AP por método\n",
    "df_en['method_format_tipo'] = df_en['method_format'] + \" + \" + df_en['tipo']\n",
    "df_en = pd.concat([df_en, df_dive])\n",
    "plt.figure(figsize=(20,8))\n",
    "ax = sns.barplot(x='dataset' ,hue=\"method_format_tipo\", y=\"AP\", ci=\"sd\",data=df_en)\n",
    "ax.set(xlabel=\"Dataset\")\n",
    "ax.set_title(f\"AP nos datasets, cada dataset usa combinações geradas usando os {best_pattern_num_train} melhores\"\n",
    "             f\" padrões do DEV\")\n",
    "ax.legend(bbox_to_anchor=(1, 1))\n",
    "legend = ax.get_legend()\n",
    "_ = plt.xticks(rotation=75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8)) # this creates a figure 8 inch wide, 4 inch high\n",
    "\n",
    "g = sns.catplot(x=\"method_format_tipo\", y=\"AP\", col=\"dataset\", col_wrap=5,\n",
    "                data=df_en, saturation=.5,\n",
    "                kind=\"bar\", ci=\"sd\", aspect=.9, legend='full')\n",
    "(g.set_axis_labels(\"\", \"AP\")\n",
    "    .set_xticklabels(rotation=40, ha=\"right\")\n",
    "    .set_titles(\"{col_name}\"))\n",
    "plt.subplots_adjust(top=0.9, hspace=0.3)\n",
    "_ = g.fig.suptitle('AP para cada método')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print contagem\n",
    "df_print = df_ratios.groupby(['dataset', 'tipo']).sum()[['true', 'false', 'total']]\n",
    "df_print['ratio'] = df_print['true'] / df_print['total']\n",
    "df_print"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}